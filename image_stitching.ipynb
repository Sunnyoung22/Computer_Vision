{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9131c53ea609b1c83a4930f9ef9b895156b0f40bc80fb8f9ee0bf5a21c6927cd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computer Vision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2736, 3648, 3)\n(547, 730, 3)\n"
    }
   ],
   "source": [
    "## 2020.10.4 Sunnyoung\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "picl_name = \".\\\\data\\\\Boat_a.jpg\"\n",
    "picr_name = \".\\\\data\\\\Boat_b.jpg\"\n",
    "# picl_name = \".\\\\data\\\\City_a.jpg\"\n",
    "# picr_name = \".\\\\data\\\\City_b.jpg\"\n",
    "\n",
    "# Read the pictures that needed to solve\n",
    "origin_img_l = cv2.imread(picl_name) \n",
    "img_left = cv2.resize(origin_img_l, (0,0), fx=0.2, fy=0.2)\n",
    "img_left_gray = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "origin_img_r = cv2.imread(picr_name)\n",
    "img_right = cv2.resize(origin_img_r, (0,0), fx=0.2, fy=0.2)\n",
    "img_right_gray = cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "print(origin_img_l.shape)\n",
    "print(img_left.shape)\n",
    "cv2.imshow(\"Boat_left\", img_left)\n",
    "cv2.imshow(\"Boat_right\", img_right)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# find the key points and descriptors with SIFT\n",
    "kp_a, dist_a = sift.detectAndCompute(img_left_gray, None)\n",
    "kp_b, dist_b = sift.detectAndCompute(img_right_gray, None)\n",
    "\n",
    "cv2.imshow(\"Feature point\", cv2.drawKeypoints(img_left, kp_a, None))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# 构造BFMatcher()蛮力匹配，匹配sift特征向量距离最近对应组分\n",
    "match = cv2.BFMatcher()\n",
    "matches = match.knnMatch(dist_a, dist_b, k=2)\n",
    "\n",
    "# 取一幅图像中的一个SIFT关键点，并找出其与另一幅图像中欧式距离最近的前两个关键点，在这两个关键点中，\n",
    "# 如果最近的距离除以次近的距离得到的比率ratio少于某个阈值T，则接受这一对匹配点。\n",
    "# d1:最近邻，d2:次近邻。即d1<k*d2。\n",
    "# 我们知道距离越近匹配度越高，但是，当所有点的距离都比较近时，匹配的可靠性不高。反之，如果只有点一个距离比较近，\n",
    "# 其它点距离都相对较远时，该点匹配的可靠度增加。d1<k*d2就是为了说明这一点。\n",
    "good_points = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.5*n.distance:\n",
    "        good_points.append(m)\n",
    "\n",
    "# imageA和imageB表示图片，kpsA和kpsB表示关键点， matches表示进过cv2.BFMatcher获得的匹配的索引值，也有距离， flags表示有几个图像\n",
    "img3 = cv2.drawMatches(img_left, kp_a, img_right, kp_b, good_points, None, flags = 2)\n",
    "cv2.imshow(\"Feature point matching\",img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCH_COUNT = 10\n",
    "if len(good) > MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    h,w = img_left_gray.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, M)\n",
    "    img_right_gray = cv2.polylines(img_right_gray,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "    #cv2.imshow(\"original_image_overlapping.jpg\", img2)\n",
    "else:\n",
    "    print(\"Not enought matches are found - %d/%d\", (len(good)/MIN_MATCH_COUNT))\n",
    "dst = cv2.warpPerspective(img_left,M,(img_right.shape[1] + img_left.shape[1], img_right.shape[0]))\n",
    "dst[0:img_left.shape[0],0:img_left.shape[1]] = img_left\n",
    "cv2.imshow(\"original_image_stitched.jpg\", dst)\n",
    "def trim(frame):\n",
    "    #crop top\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    #crop top\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    #crop top\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    #crop top\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "cv2.imshow(\"original_image_stitched_crop.jpg\", trim(dst))\n",
    "#cv2.imsave(\"original_image_stitched_crop.jpg\", trim(dst))\n",
    "cv2.waitKey(0)"
   ]
  }
 ]
}