{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit (conda)",
   "display_name": "Python 3.7.9 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9131c53ea609b1c83a4930f9ef9b895156b0f40bc80fb8f9ee0bf5a21c6927cd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Computer Vision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "## 2020.10.4 Sunnyoung\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "picl_name = \".\\\\data\\\\Boat_a.jpg\"\n",
    "picr_name = \".\\\\data\\\\Boat_b.jpg\"\n",
    "# picl_name = \".\\\\data\\\\City_a.jpg\"\n",
    "# picr_name = \".\\\\data\\\\City_b.jpg\"\n",
    "\n",
    "# Read the pictures that needed to solve\n",
    "origin_img_l = cv2.imread(picl_name) \n",
    "img_left = cv2.resize(origin_img_l, (0,0), fx=0.2, fy=0.2)\n",
    "img_left_gray = cv2.cvtColor(img_left, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "origin_img_r = cv2.imread(picr_name)\n",
    "img_right = cv2.resize(origin_img_r, (0,0), fx=0.2, fy=0.2)\n",
    "img_right_gray = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# print(origin_img_l.shape)\n",
    "# print(img_left.shape)\n",
    "cv2.imshow(\"Boat_left\", img_left)\n",
    "cv2.imshow(\"Boat_right\", img_right)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# find the key points and descriptors with SIFT\n",
    "kp_a, dist_a = sift.detectAndCompute(img_left_gray, None)\n",
    "kp_b, dist_b = sift.detectAndCompute(img_right_gray, None)\n",
    "\n",
    "cv2.imshow(\"Feature point\", cv2.drawKeypoints(img_left, kp_a, None))\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造BFMatcher()蛮力匹配，匹配sift特征向量距离最近对应组分\n",
    "match = cv2.BFMatcher()\n",
    "matches = match.knnMatch(dist_a, dist_b, k=2)\n",
    "\n",
    "# 取一幅图像中的一个SIFT关键点，并找出其与另一幅图像中欧式距离最近的前两个关键点，在这两个关键点中，\n",
    "# 如果最近的距离除以次近的距离得到的比率ratio少于某个阈值T，则接受这一对匹配点。\n",
    "# d1:最近邻，d2:次近邻。即d1<k*d2。\n",
    "# 我们知道距离越近匹配度越高，但是，当所有点的距离都比较近时，匹配的可靠性不高。反之，如果只有点一个距离比较近，\n",
    "# 其它点距离都相对较远时，该点匹配的可靠度增加。d1<k*d2就是为了说明这一点。\n",
    "good_points = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.5*n.distance:\n",
    "        good_points.append(m)\n",
    "\n",
    "# imageA和imageB表示图片，kpsA和kpsB表示关键点， matches表示进过cv2.BFMatcher获得的匹配的索引值，也有距离， flags表示有几个图像\n",
    "img3 = cv2.drawMatches(img_left, kp_a, img_right, kp_b, good_points, None, flags = 2)\n",
    "cv2.imshow(\"Feature point matching\",img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "MIN_MATCH_COUNT = 10\n",
    "if len(good_points) > MIN_MATCH_COUNT:\n",
    "    dst_pts = np.float32([kp_a[m.queryIdx].pt for m in good_points]).reshape(-1,1,2)\n",
    "    src_pts = np.float32([kp_b[m.trainIdx].pt for m in good_points]).reshape(-1,1,2)\n",
    "    # 在这个函数参数中，输入的m1和m2是两个对应的序列，这两组序列的每一对数据一一匹配，其中既有正确的匹配，也有错误的匹配，\n",
    "    # 正确的可以称为内点，错误的称为外点，RANSAC方法就是从这些包含错误匹配的数据中，分离出正确的匹配，并且求得单应矩阵。\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    h,w = img_right_gray.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts, M)\n",
    "    img_left_gray = cv2.polylines(img_left_gray,[np.int32(dst)],True,0,3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"original_image_overlapping.jpg\", img_left_gray)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Not enought matches are found - %d/%d\", (len(good)/MIN_MATCH_COUNT))\n",
    "\n",
    "dst = cv2.warpPerspective(img_right,M,(img_right.shape[1] + img_left.shape[1], img_left.shape[0]))\n",
    "cv2.imshow(\"original_image_stitched.jpg\", dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "dst[0:img_left.shape[0],0:img_left.shape[1]] = img_left\n",
    "cv2.imshow(\"original_image_stitched.jpg\", dst)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(frame):\n",
    "    #crop top\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    #crop top\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    #crop top\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    #crop top\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "cv2.imshow(\"original_image_stitched_crop.jpg\", trim(dst))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "TEST_FOLDER = \".\\\\data\\\\smiling_cropped\\\\\"\n",
    "\n",
    "IMAGE_SIZE = (60,60)\n",
    "test_image = cv2.imread(TEST_FOLDER +\"1.png\")\n",
    "test_image=cv2.resize(test_image,IMAGE_SIZE,interpolation=cv2.INTER_CUBIC)\n",
    "test_image2 = cv2.imread(TEST_FOLDER +\"2.png\")\n",
    "test_image2=cv2.resize(test_image2,IMAGE_SIZE,interpolation=cv2.INTER_CUBIC)\n",
    "imgs = np.hstack([test_image,test_image2])\n",
    "cv2.imshow(\"12.............\",imgs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}